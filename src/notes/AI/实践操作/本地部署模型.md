# 本地部署模型



Ollama 部署

```
# 安装 ollama 客户端
curl -fsSL https://ollama.com/install.sh | sh

# systemclt 管理服务
systemctl status/stop/start ollama

# 下载模型
# 模型市场 https://ollama.com/search
ollama pull qwen3:0.6b

# 运行模型
ollama run qwen3:0.6b

# 交互
curl http://localhost:11434/api/chat -d '{
  "model": "qwen3:0.6b",
  "messages": [{
    "role": "user",
    "content": "你好，你是谁？"
  }],
  "stream": false
}'

curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:0.6b",
  "prompt": "你好，你是谁？"
}'
# 交互
curl -X POST http://localhost:11434/api/chat -H 'Content-Type: application/json' --data-binary @r.json
```

参考

https://zhuanlan.zhihu.com/p/22439948590

https://ollama.com/

https://ollama.com/search