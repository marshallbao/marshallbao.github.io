# 学习 AI

## LLM 基本原理

### 核心工作步骤

1. 将单词转换成词向量（word vector），向量的内容用数字组合来表示；单词的多少用 token（词元） 来计算
2. 多层解析和预测
LLM 里设置了多层或多级的语言规则理解器；GPT-3（第三代GPT）中，包含了96层的 Transformer，参数总量高达1750亿个！
3. 使用大数据训练模型，提高准确度
使大语言模型（LLM）接触到超级大量（几百亿）来自互联网的文本，通过阅读和分析这些文本，LLM可以识别单词和句子的使用方式，它们的含义，以及它们之间的关系，从而不断提高语言的准确度
GPT-3 是在大约5000亿个单词的语料库上进行训练的。相比之下，一个典型的人类儿童到10岁时，大约会接触到 1 亿个单词左右

我的理解

LLM 实际上就是根据输入一直猜测并输出，直到猜测出结束符号；

或者说你可以理解为：我们的问答 + 他们的输出是一篇文章，LLM 根据猜测写出了这篇文章；

至于为什么猜测这么准？

### 其他知识

#### token

词元，语言的基本单位（单词，标点，数字，等）

#### 词向量

就是一组词用多个维度来描述，比如“狗”，的一些维度：动物 有毛 会叫 宠物 

例如，Cat（猫）这个英文单词，在LLM的数字世界里，用数字组合表示如下（这个数字组合包括多达300个数字）。

[0.0074, 0.0030, -0.0105, 0.0742, 0.0765, -0.0011, 0.0265, 0.0106, 0.0191, 0.0038, -0.0468, -0.0212, 0.0091, 0.0030, -0.0563, -0.0396, -0.0998, -0.0796, …, 0.0002]

#### Transformer（模型的核心）

Transformer 是一种深度学习模型架构, 是靠「注意力机制（Attention）」来理解和生成信息的模型。

#### Attention mechanism 注意力机制（Transformer 的核心）

是一种用于机器学习和自然语言处理的技术，它可以根据输入的信息动态地将注意力集中在不同的位置，从而使得模型能够更好地理解和处理输入的序列数据。

#### sequence-to-sequence 序列到序列

是一种常见的神经网络架构，用于将一个序列映射到另一个序列。它由两个主要组件组成：编码器和解码器。编码器将输入序列转换为一个向量表示，解码器则将这个向量解码成目标序列。在这个过程中，模型会学习到输入序列和输出序列之间的对应关系，从而实现转换。

### 参考

https://zhuanlan.zhihu.com/p/669245797

https://www.zhihu.com/people/wu-jiang-cu-kou-37



## 多模态大模型

传统的大语言模型（LLM）如GPT、LLaMA等主要处理文本序列，基于Transformer架构在自然语言处理任务上取得了巨大成功。然而，现实世界的信息是多模态的——文字、图像、音频、视频等多种形式共同构成了人类的认知输入。

多模态进化的本质问题是：如何让这个"文字专家"学会处理图像、音频等完全不同的信息形式？

大语言模型的核心是 注意力机制，它本质上是在学习 Token 之间的关系。无论 Token 来自文字、图像还是音频，注意力机制都能学会它

们之间的语义关联。所以需要设计一个统一架构方案：

通过模态特定的编码器将不同类型数据转换为统一的 Token 表示，所有 Token 在同一序列中排列，由统一的 Transformer backbone 处理。

整体流程

模态统一编码机制 --> 序列构建与处理流程  --> 注意力机制的适配



参考

https://grapecity.csdn.net/6882ec31080e555a88d227a9.html





## 基于大模型的应用技术

### 提示词

提示词（Prompt）通过向模型提供清晰、具体的指令、背景信息和示例，来引导模型生成符合用户意图的输出。其核心是将用户的需求转化为模型能够理解的“语言”，从而影响模型的输出行为，提高生成内容的准确性、可控性和效率。其核心是将用户的需求转化为模型能够理解的“语言”，从而影响模型的输出行为，提高生成内容的准确性、可控性和效率。

原理核心

- **沟通与引导**：提示词最基本的作用是沟通。它通过清晰的语言来指导模型，告诉它“做什么”，以及如何做。
- **引导模型行为**：模型本质上是通过概率预测下一个词来生成内容，模糊的指令会导致模糊的结果。清晰具体的提示词能为模型提供明确的执行路径，减少其“猜测”的空间，从而更准确地生成内容。
- **优化输出质量**：通过精心设计提示词，可以控制输出的风格、格式、细节和情感等，以获得更符合要求的输出，这比对模型本身进行微调更具成本效益。 

提示词的关键组成部分

- **指令（Instructions）**：这是提示词的核心，明确描述任务是什么，例如“总结这篇文章”或“写一封邮件”。
- **上下文（Context）**：提供与任务相关的背景信息，帮助模型更好地理解当前情境。在多轮对话中，上下文尤为重要，它能保持对话的连贯性。
- **示例（Examples）**：提供一到多个具体的例子来演示期望的输出格式和执行方式。这种“示范学习”可以显著提高输出的正确性。
- **输入（Input）**：用户需要模型处理的具体数据或信息，应该在提示词中清晰标识出来。
- **输出（Output）**：可以指定期望的输出格式，如JSON、XML或特定的文本结构，以便后续的自动化处理。 

如何通过提示词工程优化输出

- **清晰性和具体性**：避免使用模糊的词语，提供具体的细节。例如，将“说说机器学习”改为“列出三种机器学习算法并简述其优缺点”。
- **添加思维引导**：对于复杂的推理任务，可以在提示词中加入逐步推理的步骤要求，让模型“一步一步思考”，以提高准确性。
- **调整输出风格和长度**：可以明确指示模型生成简洁或详细的回答，以及所需的输出格式



### MCP

Model Context Protocol，MCP 是一个开放协议，它规范了应用程序向 LLM 提供上下文的方式

使用场景

AI agent 配置了 MCP server ，MCP server 是一个服务（守护进程/命令），有很多 MCP server ,每个 MCP server 都有自己的功能，比如天气 MCP Server;你可以问 AI agent 天气情况，而 AI agent 可以通过 天气这个 MCP Server 来获取天气情况然后返回给客户；



#### 参考

https://cloud.tencent.com/developer/article/2512423

https://zhuanlan.zhihu.com/p/27327515233

https://guangzhengli.com/blog/zh/model-context-protocol

https://cloud.tencent.com/developer/article/2504975

https://github.com/Flux159/mcp-server-kubernetes



### AI Agent

AI 智能体是使用 AI 来实现目标并代表用户完成任务的软件系统。其表现出了推理、规划和记忆能力，并且具有一定的自主性，能够自主学习、适应和做出决定。

像目前的一些大模型客户端或者 ai ide 都是不完全形态的 AI Agent。或者说他们是编程智能体

### 参考

https://cloud.google.com/discover/what-are-ai-agents?hl=zh-CN

https://guangzhengli.com/blog/zh/model-context-protocol

https://guangzhengli.com/blog/zh/gpt-embeddings



AI Code IDE/代码补全

”基于上下文编程“

Cursor 

使用 RAG 技术将整个项目进行索引，并以语义(向量)搜索的方式给 LLM 提供整个项目的上下文。-->

Cursor 会在你新打开项目的时候，将整个代码库在本地拆分为多个小块内容，然后再上传到 Cursor 云服务器中，使用 embedding 模型来 embeds 代码，并存储到云向量数据库中。接着你在 Chat/Agent 中提问时，Cursor 会在推理时对 prompt 进行嵌入，让 Turbopuffer 进行最近邻搜索，将混淆后的文件路径和行范围发送回 Cursor 客户端，并在客户端本地读取这些文件代码块。

Claude Code 

每次一上来就先通过终端命令开始分析代码库的项目结构和基本的技术栈信息，这与 Cursor 关注点在具体的任务和少数的代码文件中不同，Claude Code 以一种更全局的视角先分析项目整体情况，然后再开始开发。虽然这肯定是更加消耗 tokens ，但是有了这些项目整体信息后，Claude Code 编写的代码确实更加符合项目原本的开发模式和编码规范。

加上 Claude Code 选择了一套和 Cursor 完全不同的检索代码上下文的方案，那就是基于 Unix 工具的检索方案。 例如使用 grep, find, git, cat 等等终端命令而不是 RAG 的方案。

这种方案一般来讲更加符合程序员的编程习惯，例如程序员在做某个编程任务时，如果不熟悉代码，一般会从某个关键方法名或者对象名称开始，一层一层的开始模糊搜索或者正则搜索。直到找全了业务相关的代码，再开始编程。

Claude Code 选择的就是这种上下文工程模式，在你提问之后，基于你的提问进行关键词不断检索，直到找到项目中所有需要的上下文代码，然后再开始编程，亦或者是一轮一轮的对话、编程和检索，一直重复这个过程，直到 LLM 认为找全了上下文。



参考

https://guangzhengli.com/blog/zh/vibe-coding-and-context-coding

https://www.shengwang.cn/blog/blogdetail/free-ai-code-assistant/

https://juejin.cn/post/7494549597605576754

