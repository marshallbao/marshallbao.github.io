# Transformer 

## 🧠 一、Transformer 是什么？

Transformer 是一种**深度学习模型架构**，最早由 Google 在 2017 年的论文
 👉《Attention is All You Need》提出。
 它现在是大模型（比如 GPT、Claude、Gemini 等）的**核心基础**。

一句话总结：

> Transformer 是靠「注意力机制（Attention）」来理解和生成信息的模型。

------

## 🔍 二、核心结构总览

Transformer 通常包含两个部分：

| 模块                  | 功能                               |
| --------------------- | ---------------------------------- |
| **Encoder（编码器）** | 理解输入，比如一句话或一段文字     |
| **Decoder（解码器）** | 生成输出，比如翻译、回答或续写文本 |

> GPT 只用 **Decoder** 部分（专门做生成任务）。
>  BERT 只用 **Encoder** 部分（专门做理解任务）。

------

## 🧩 三、关键术语讲解

### 1. Token（标记）

模型不是直接读文字，而是把文字拆成「词片」：

```
"Transformer模型很厉害"
→ ["Trans", "former", "模型", "很", "厉害"]
```

这些词片就叫 **token**。
 模型看到的是 token 的序列，比如 `[101, 3985, 2593, ...]`（每个 token 有编号）。

------

### 2. Embedding（向量表示）

每个 token 都会被映射成一个高维向量，比如：

```
"猫" → [0.2, -0.5, 0.9, ...]
```

这些向量捕捉语义，比如“猫”和“狗”在向量空间里会很接近。

------

### 3. Positional Encoding（位置编码）

Transformer 不像 RNN 那样有顺序感，所以要人工告诉它「谁在前谁在后」。

例子：

> “我 爱 你”
>  位置编码让模型知道 “我”在第一个位置，“你”在第三个位置。

------

### 4. Attention（注意力机制）⭐️⭐️⭐️

这是 Transformer 的灵魂。

简单理解：

> 模型在处理一个词时，会去「关注」其他相关的词。

例如句子：

> “小明打了小刚，因为他生气了。”

模型在理解“他”时，会计算：

- “他”和“小明”的相关度
- “他”和“小刚”的相关度

然后根据注意力权重判断“他”指谁。

这就是 **Self-Attention（自注意力）**。

------

### 5. Query / Key / Value （Q/K/V）

注意力机制的数学基础：

| 名称      | 类比         | 作用               |
| --------- | ------------ | ------------------ |
| **Query** | 当前词的问题 | 我想关注谁？       |
| **Key**   | 每个词的标签 | 我是否与别人相关？ |
| **Value** | 实际信息内容 | 我提供什么信息？   |

Self-Attention 会算出 “Query 与 Key” 的相似度，然后加权汇总所有 Value。

------

### 6. Multi-Head Attention（多头注意力）

不是只计算一组 Q/K/V，而是多组并行计算。
 每个「头」关注不同类型的关系：

- 一个头可能关注「语法关系」
- 一个头可能关注「语义关系」

最后把它们合并起来。

------

### 7. Feed Forward（前馈网络）

每个 token 经过注意力后，还会通过一个小型的全连接神经网络来“再加工”一下。

------

### 8. Layer Normalization（层归一化）

类似于给每一层的输出做“标准化处理”，防止梯度不稳定。

------

### 9. Residual Connection（残差连接）

每层输出都加上输入（shortcut），让模型更容易训练。

------

### 10. Mask（掩码）

在生成任务中（如 GPT），模型不能看到未来的词。
 所以要用「掩码」屏蔽掉未来的信息，让它只看前面的 token。

------

## 🧱 四、Transformer 层结构（简化图）

```
[输入 Token]
     ↓
Embedding + Positional Encoding
     ↓
┌──────────────────────┐
│ Multi-Head Attention │
├──────────────────────┤
│ Feed Forward Network │
└──────────────────────┘
     ↓
[输出 Token 表示]
```

------

## 🧮 五、训练和推理

- **训练阶段**：模型看大量文本，学习预测下一个 token。
- **推理阶段**：模型用前面的 token 不断生成后续 token，一次一个。

例如：

```
输入: "机器学习是"
输出: "机器学习是人工智能的一个分支。"
```

------

## 🎯 六、总结一句话

> Transformer 就是一种能「通过注意力理解上下文关系」的神经网络架构。
>  它是现代大语言模型的“引擎”，让模型既能理解，又能生成自然语言。